{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "This notebook takes a trained BAMS (https://multiscale-behavior.github.io/) model and pose data (DeepLabCut '.csvs') and returns features for behavorial classification/ studies of dynamic movement. \n",
    "The embeddings are shape (n samples, n frames per sample, n bams features) where samples = DLC csvs. Frame level embeddings, where each frame from the pose \n",
    "'.csvs' is its own datapoint, are shape (n samples * n frames per sample, n bams features). These frame level data points are embedded into a 3D space using UMAP and clustered in the UMAP \n",
    "space using DBSCAN. Sequence level embeddings can also be computed by averaging each feature over the frames of the sample and are shape (n samples, n bams features).\n",
    "\n",
    "Created by:\n",
    "Mary Beth Cassity @ mary.beth.cassity@cornell.edu\n",
    "Sarvestani Lab, Cornell University\n",
    "\n",
    "Last updated: \n",
    "9/13/2024\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get BAMS feature embeddings from trained BAMS model + pose (DLC csvs)\n",
    "#### Outline:\n",
    "#### 1. Load the data and trained model and get an embedding\n",
    "##### You can stop after this step if you just want to get the bams feature embeddings in 64 (short and long)/128 (all) dimensional space. The next steps reduce the dimensions (UMAP) and cluster the datapoints in the 3d UMAP space (DBSCAN)\n",
    "#### 2. Use UMAP to reduce dimensions and DBSCAN to cluster in UMAP space for frame level embeddings\n",
    "#### 3. Save gifs for data points in UMAP space organized by cluster (view the movement latents)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Import required libraries and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from bams.data import KeypointsDataset\n",
    "from bams.models import BAMS\n",
    "from bams import compute_representations\n",
    "from custom_dataset_w_labels import load_data, load_annotations\n",
    "\n",
    "import numpy as np\n",
    "import os \n",
    "import seaborn as sns\n",
    "import umap\n",
    "from sklearn.cluster import DBSCAN\n",
    "import pandas as pd\n",
    "\n",
    "from extra_fun import plot_3d_umap, save_gifs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load the data and trained model and get an embedding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:Red; font-size:24px;\">User input</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "input the path to the bams model folder and the dlc data folder \n",
    "\n",
    "the bams model folder should have a bams model (ending in '.pt') in it\n",
    "the code recognizes the model by it beginning with 'bams-custom' and ending with '.pt' therefore, it is important to make sure that only one exists per folder\n",
    "\n",
    "the data folder should be organized with subfolders named with the species (label)\n",
    "each subfolder contains dlc csvs. each csv should contain the same number of datapoints (frames) \n",
    "'''\n",
    "\n",
    "feature_processing = 'subtract_centroid' \n",
    "\n",
    "### input path to model folder here ###\n",
    "### change the model to match what features you want to extract-- see the ppt Documentation for model:feature match ###\n",
    "model_folder = r\"X:\\MaryBeth\\BAMS\\Visuomotor-Latents\\models\\bams-custom-2024-08-29-15-48-48_0.8\" \n",
    "\n",
    "### input path to data folder here ###\n",
    "### you don't need to change this unless you want to change the input dataset ###\n",
    "dlc_data_folder = r\"X:\\MaryBeth\\BAMS\\Visuomotor-Latents\\data\\threshold_0.8\\movement\" "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:Red; font-size:24px;\">User input</span>\n",
    "##### load an embedding (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### If you want to load a previously computed embedding, uncomment the line below ###\n",
    "\n",
    "# embeddings = torch.load(os.path.join(model_folder,'embeddings.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with os.scandir(model_folder) as entries:\n",
    "    for entry in entries:\n",
    "        if entry.is_file() and entry.name.startswith('bams-custom') and entry.name.endswith('.pt'):\n",
    "            model_name = entry\n",
    "            print(\"Loading model\", model_name)\n",
    "            print()\n",
    "            \n",
    "model_path = os.path.join(model_folder, model_name)\n",
    "annotations_path = os.path.join(model_folder,\"video_labels.csv\")\n",
    "\n",
    "hoa_bins = 32\n",
    "model_input = load_data(dlc_data_folder, model_folder, feature_processing = feature_processing, create_csv = False) ### set to True if you want to re create the mapping from bams to the original data csvs-- you only need to do this if you want to change the input dataset ### \n",
    "annotations, eval_utils = load_annotations(annotations_path)\n",
    "\n",
    "dataset = KeypointsDataset(\n",
    "        keypoints=model_input,\n",
    "        cache=False,\n",
    "        hoa_bins=hoa_bins,\n",
    "        annotations=annotations,\n",
    "        eval_utils=eval_utils\n",
    "    )\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "### initiate the model ###\n",
    "\n",
    "model = BAMS(\n",
    "        input_size=dataset.input_size,\n",
    "        short_term=dict(num_channels=(64, 64, 64, 64), kernel_size=3),\n",
    "        long_term=dict(num_channels=(64, 64, 64, 64, 64), kernel_size=3, dilation=4),\n",
    "        predictor=dict(\n",
    "            hidden_layers=(-1, 256, 512, 512, dataset.target_size * hoa_bins)\n",
    "        ),\n",
    "    ).to(device)\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "model.eval()\n",
    "\n",
    "### compute the embeddings ###\n",
    "embeddings = compute_representations(model, dataset, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' \n",
    "each embedding (short, long, all) should be shape (n samples (total csvs), n frames per sample, n bams features)\n",
    "\n",
    "frame level embeddings (n samples * n frames per sample, n bams features) are used to plot in 3d UMAP space and save datapoints (frames) as gifs\n",
    "'''\n",
    "\n",
    "### retrieve embeddings ###\n",
    "\n",
    "short_term = embeddings['short_term']\n",
    "long_term = embeddings['long_term']\n",
    "all_embeddings = torch.cat([short_term, long_term], dim=2)\n",
    "\n",
    "print(\"short_term: \", np.shape(short_term))\n",
    "print(\"long_term: \", np.shape(long_term))\n",
    "print(\"all_embeddings: \", np.shape(all_embeddings))\n",
    "print(\"\")\n",
    "\n",
    "### compute sequence level embeddings of the bams features by averaging over the frames for each sample ###\n",
    "\n",
    "short_term_seq = torch.mean(short_term, dim=1, keepdim=False)\n",
    "long_term_seq = torch.mean(long_term, dim=1, keepdim=False)\n",
    "all_embeddings_seq = torch.cat([short_term_seq, long_term_seq], dim=1)\n",
    "\n",
    "print(\"short_term seq: \", np.shape(short_term_seq))\n",
    "print(\"long_term seq: \", np.shape(long_term_seq))\n",
    "print(\"all_embeddings: \", np.shape(all_embeddings_seq))\n",
    "print(\"\")\n",
    "\n",
    "### reshape frame array to get frame level embeddings ###\n",
    "\n",
    "short_term_frame = short_term.view(short_term.size(0) * short_term.size(1), short_term.size(2))\n",
    "long_term_frame = long_term.view(long_term.size(0) * long_term.size(1), long_term.size(2))\n",
    "all_embeddings_frame = all_embeddings.view(all_embeddings.size(0) * all_embeddings.size(1), all_embeddings.size(2))\n",
    "\n",
    "print(\"short_term frame: \", np.shape(short_term_frame))\n",
    "print(\"long_term frame: \", np.shape(long_term_frame))\n",
    "print(\"all_embeddings frame: \", np.shape(all_embeddings_frame))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:Red; font-size:24px;\">User input</span>\n",
    "##### save the embedding (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### If you want to save a newly computed embedding, uncomment the line below ###\n",
    "\n",
    "# torch.save({'short_term': short_term, 'long_term': long_term}, os.path.join(model_folder,'embeddings.pth'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Use UMAP to reduce dimensions and DBSCAN to cluster in UMAP space for frame level embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:Red; font-size:24px;\">User input</span>\n",
    "##### choose which embedding you want to investigate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Frame level embeddings: each frame from the pose csvs is its own datapoint: shape (n samples * n frames per sample, n bams features) \n",
    "'''\n",
    "\n",
    "### choose which embedding you want to investigate (short, long, or all) ###\n",
    "\n",
    "embedding = short_term # long_term, all_embeddings\n",
    "embedding_frame = short_term_frame # long_term_frame, all_embeddings_frame\n",
    "embedding_frame_name = \"short_term_frame\" # \"long_term_frame\", \"all_embeddings_frame\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### create mapping from bams embeddings to the video/ csv space ###\n",
    "\n",
    "### read the order in which the csvs were passed to bams ###\n",
    "df = pd.read_csv(annotations_path)\n",
    "video = df['video_name']\n",
    "\n",
    "### for each csv, repeat its name n frames per sample times ###\n",
    "video = np.repeat(video, embedding.shape[1])\n",
    "print(np.shape(video))\n",
    "\n",
    "### for each csv, number the frames from 0 to n frames per sample ### \n",
    "repeated_array = np.tile(np.arange(embedding.shape[1]), embedding.shape[0])\n",
    "print(np.shape(repeated_array))\n",
    "\n",
    "### make mapping that contains [video name, frame number] for each sample ###\n",
    "video_frames = np.column_stack((video, repeated_array))\n",
    "print(np.shape(video_frames))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:Red; font-size:24px;\">User input</span>\n",
    "##### choose how many datapoins to randomly sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "due to the large size of the frame level dataset, a random sample of the data is selected to give to umap and dbscan\n",
    "'''\n",
    "\n",
    "### choose how many samples you want to plot ###\n",
    "sample_size = 4000\n",
    "\n",
    "os.makedirs(os.path.join(model_folder,f\"{sample_size}\"), exist_ok=True)\n",
    "\n",
    "### set the random state ###\n",
    "np.random.seed(42)\n",
    "sample_indices = np.random.choice(len(embedding_frame), size=sample_size, replace=False)\n",
    "\n",
    "sample = embedding_frame[sample_indices]\n",
    "video_frames_sample = video_frames[sample_indices]\n",
    "\n",
    "umap_model = umap.UMAP(n_components=3, random_state=42) \n",
    "behavior_umap = umap_model.fit_transform(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:Red; font-size:24px;\">User input</span>\n",
    "##### choose eps and min_samples for DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "the value of eps can be computed, however, I have found this doesn't work well for our purpose/ dataset composition\n",
    "see section 4.1 of https://dl.acm.org/doi/pdf/10.1145/3068335 for a description of the knee method for choosing eps\n",
    "and strategies for choosing min_samples\n",
    "'''\n",
    "\n",
    "### set eps and min_samples to a computed or  experimentally determined value ###\n",
    "eps = 0.15\n",
    "min_samples = 5\n",
    "\n",
    "dbscan_model = DBSCAN(eps=eps, min_samples=min_samples) \n",
    "dbscan_labels = dbscan_model.fit_predict(behavior_umap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### plot the 3d umap using plotly ###\n",
    "plot_3d_umap(dbscan_labels, behavior_umap, video_frames_sample, model_folder, sample_size, eps, min_samples, embedding_frame_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Save gifs for data points in UMAP space organized by cluster (view the movement latents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mp4_folder = r\"X:\\MaryBeth\\BAMS\\Visuomotor-Latents\\data\\threshold_0.8\\0.8_mp4s_combined\"\n",
    "save_gifs(model_folder, eps, min_samples, video_frames_sample, dbscan_labels, mp4_folder, sample_size, embedding_frame_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
